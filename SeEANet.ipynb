{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlk(nn.Module):\n",
    "    def __init__(self, inch, outch):\n",
    "        super(InceptionBlk, self).__init__()\n",
    "        self.b1_conv1 = nn.Conv1d(inch, outch, kernel_size=1)\n",
    "        self.b1_lrelu = nn.LeakyReLU(negative_slope=0.3)\n",
    "        \n",
    "        self.b2_conv1 = nn.Conv1d(inch, outch, kernel_size=1)\n",
    "        self.b2_lrelu1 = nn.LeakyReLU(negative_slope=0.3)\n",
    "        self.b2_conv2 = nn.Conv1d(outch, outch, kernel_size=5, padding=2)\n",
    "        self.b2_lrelu2 = nn.LeakyReLU(negative_slope=0.3)\n",
    "        \n",
    "        self.b3_conv1 = nn.Conv1d(inch, outch, kernel_size=1)\n",
    "        self.b3_lrelu1 = nn.LeakyReLU(negative_slope=0.3)\n",
    "        self.b3_conv2 = nn.Conv1d(outch, outch, kernel_size=9, padding=4)\n",
    "        self.b3_lrelu2 = nn.LeakyReLU(negative_slope=0.3)\n",
    "        \n",
    "        self.b4_mpool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "        self.b4_conv1 = nn.Conv1d(inch, outch, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.b1_conv1(x)\n",
    "        x1 = self.b1_lrelu(x1)\n",
    "\n",
    "        x2 = self.b2_conv1(x)\n",
    "        x2 = self.b2_lrelu1(x2)\n",
    "        x2 = self.b2_conv2(x2)\n",
    "        x2 = self.b2_lrelu2(x2)\n",
    "\n",
    "        x3 = self.b3_conv1(x)\n",
    "        x3 = self.b3_lrelu1(x3)\n",
    "        x3 = self.b3_conv2(x3)\n",
    "        x3 = self.b3_lrelu2(x3)\n",
    "\n",
    "        x4 = self.b4_mpool(x)\n",
    "        x4 = self.b4_conv1(x4)\n",
    "\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardSwish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HardSwish, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.clamp(F.relu(x + 3.0), max=6.0) / 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstConvBlk(nn.Module):\n",
    "        def __init__(self, outch, input_shape = [256, 6]):\n",
    "            \n",
    "            super(FirstConvBlk, self).__init__()        \n",
    "            # Shallow feature extraction module\n",
    "            self.first_conv_blocks = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(1, outch, kernel_size=9, padding=4),\n",
    "                    nn.LeakyReLU(negative_slope=0.3),\n",
    "                    nn.MaxPool1d(4),\n",
    "                    nn.Dropout(0.2)\n",
    "                ) for _ in range(input_shape[1])\n",
    "                ])\n",
    "        def forward(self, x):\n",
    "            first_conv_outs = []\n",
    "            for i in range(x.shape[1]):\n",
    "                x_input = x[:, i:i+1, :]  # Slice operation\n",
    "                out = self.first_conv_blocks[i](x_input)\n",
    "                first_conv_outs.append(out)\n",
    "        \n",
    "            x = torch.cat(first_conv_outs, dim=1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelSpilit(nn.Module):\n",
    "    def __init__(self, groups=4):\n",
    "        super(ChannelSpilit, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, filters, width = x.shape\n",
    "        channels_per_group = filters // self.groups\n",
    "        x = x.view(-1, channels_per_group, width)\n",
    "        x1, x2 = torch.split(x, x.size(1) // 2, dim=1)\n",
    "        return x1, x2\n",
    "\n",
    "class ChannelShuffler(torch.nn.Module):\n",
    "    def __init__(self, num, groups=2):\n",
    "        super(ChannelShuffler, self).__init__()\n",
    "        self.num = num\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, w = x.shape\n",
    "        x = x.reshape(b, self.groups, -1, w)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(b, -1, w)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.fc = nn.Linear(channels, channels, bias=True)\n",
    "        self.fc.weight.data.zero_()\n",
    "        self.fc.bias.data.fill_(1)\n",
    "        self.activation = nn.Hardsigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width = x.shape\n",
    "        x_global_avg_pool = torch.mean(x, dim=2)\n",
    "        y = self.fc(x_global_avg_pool)\n",
    "        y = self.activation(y)\n",
    "        y = y.view(batch_size, channels, 1)\n",
    "        output = x * y\n",
    "        return output\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.group_norm = nn.GroupNorm(channels, channels)\n",
    "        self.fc = nn.Linear(64, 64, bias=True)\n",
    "        self.activation = nn.Hardsigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_grop_norm = self.group_norm(x)\n",
    "        x_global_avg_pool = torch.mean(x_grop_norm, dim=1)\n",
    "        y = self.fc(x_global_avg_pool)\n",
    "        y = self.activation(y)\n",
    "        y = y.view(y.size(0), 1, y.size(1))\n",
    "        output = x * y\n",
    "        return output\n",
    "\n",
    "\n",
    "class SABlk(nn.Module):\n",
    "    def __init__(self, groups, num):\n",
    "        super(SABlk, self).__init__()\n",
    "        self.num = num\n",
    "        self.channel_spilit = ChannelSpilit(groups)\n",
    "        self.cam = ChannelAttention(8)\n",
    "        self.sam = SpatialAttention(8)\n",
    "        self.channel_shuffle = ChannelShuffler(num, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel, len = x.shape[1:]\n",
    "        x1, x2 = self.channel_spilit(x)\n",
    "        x1 = self.cam(x1)\n",
    "        x2 = self.sam(x2)\n",
    "        y = torch.cat([x1, x2], dim=1)\n",
    "        y = y.view(-1, channel, len)\n",
    "        y = self.channel_shuffle(y)\n",
    "\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedBottleneck(nn.Module):\n",
    "    def __init__(self, ImproverFilter, kernel, e, num=1, groups=8, alpha=1.0):\n",
    "        super(ImprovedBottleneck, self).__init__()\n",
    "        cchannel = int(alpha * ImproverFilter)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.LazyConv1d(e, kernel_size=1),\n",
    "            nn.BatchNorm1d(e),\n",
    "            HardSwish()\n",
    "        )\n",
    "        self.depthwise_conv = nn.LazyConv1d(out_channels=128, kernel_size=kernel, stride=1, padding=4, groups=128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.hs1 = HardSwish()\n",
    "        self.sa = SABlk(groups, num)\n",
    "        self.conv1 = nn.LazyConv1d(cchannel, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=cchannel)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        init_filters = x.shape[1]\n",
    "        y = self.conv_block(x)\n",
    "        y = self.depthwise_conv(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.hs1(y)\n",
    "        y = self.sa(y)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn2(y)\n",
    "\n",
    "        if y.shape[1] == init_filters:\n",
    "            y += x\n",
    "\n",
    "        return y\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 8, 256]              80\n",
      "         LeakyReLU-2               [-1, 8, 256]               0\n",
      "         MaxPool1d-3                [-1, 8, 64]               0\n",
      "           Dropout-4                [-1, 8, 64]               0\n",
      "            Conv1d-5               [-1, 8, 256]              80\n",
      "         LeakyReLU-6               [-1, 8, 256]               0\n",
      "         MaxPool1d-7                [-1, 8, 64]               0\n",
      "           Dropout-8                [-1, 8, 64]               0\n",
      "            Conv1d-9               [-1, 8, 256]              80\n",
      "        LeakyReLU-10               [-1, 8, 256]               0\n",
      "        MaxPool1d-11                [-1, 8, 64]               0\n",
      "          Dropout-12                [-1, 8, 64]               0\n",
      "           Conv1d-13               [-1, 8, 256]              80\n",
      "        LeakyReLU-14               [-1, 8, 256]               0\n",
      "        MaxPool1d-15                [-1, 8, 64]               0\n",
      "          Dropout-16                [-1, 8, 64]               0\n",
      "           Conv1d-17               [-1, 8, 256]              80\n",
      "        LeakyReLU-18               [-1, 8, 256]               0\n",
      "        MaxPool1d-19                [-1, 8, 64]               0\n",
      "          Dropout-20                [-1, 8, 64]               0\n",
      "           Conv1d-21               [-1, 8, 256]              80\n",
      "        LeakyReLU-22               [-1, 8, 256]               0\n",
      "        MaxPool1d-23                [-1, 8, 64]               0\n",
      "          Dropout-24                [-1, 8, 64]               0\n",
      "     FirstConvBlk-25               [-1, 48, 64]               0\n",
      "           Conv1d-26               [-1, 48, 64]           2,352\n",
      "        LeakyReLU-27               [-1, 48, 64]               0\n",
      "           Conv1d-28               [-1, 48, 64]           2,352\n",
      "        LeakyReLU-29               [-1, 48, 64]               0\n",
      "           Conv1d-30               [-1, 48, 64]          11,568\n",
      "        LeakyReLU-31               [-1, 48, 64]               0\n",
      "           Conv1d-32               [-1, 48, 64]           2,352\n",
      "        LeakyReLU-33               [-1, 48, 64]               0\n",
      "           Conv1d-34               [-1, 48, 64]          20,784\n",
      "        LeakyReLU-35               [-1, 48, 64]               0\n",
      "        MaxPool1d-36               [-1, 48, 64]               0\n",
      "           Conv1d-37               [-1, 48, 64]           2,352\n",
      "     InceptionBlk-38              [-1, 192, 64]               0\n",
      "        LeakyReLU-39              [-1, 192, 64]               0\n",
      "           Conv1d-40              [-1, 128, 64]          24,704\n",
      "      BatchNorm1d-41              [-1, 128, 64]             256\n",
      "        HardSwish-42              [-1, 128, 64]               0\n",
      "           Conv1d-43              [-1, 128, 64]           1,280\n",
      "      BatchNorm1d-44              [-1, 128, 64]             256\n",
      "        HardSwish-45              [-1, 128, 64]               0\n",
      "    ChannelSpilit-46              [[-1, 8, 64]]               0\n",
      "           Linear-47                    [-1, 8]              72\n",
      "      Hardsigmoid-48                    [-1, 8]               0\n",
      " ChannelAttention-49                [-1, 8, 64]               0\n",
      "        GroupNorm-50                [-1, 8, 64]              16\n",
      "           Linear-51                   [-1, 64]           4,160\n",
      "      Hardsigmoid-52                   [-1, 64]               0\n",
      " SpatialAttention-53                [-1, 8, 64]               0\n",
      "  ChannelShuffler-54              [-1, 128, 64]               0\n",
      "            SABlk-55              [-1, 128, 64]               0\n",
      "           Conv1d-56              [-1, 192, 64]          24,768\n",
      "      BatchNorm1d-57              [-1, 192, 64]             384\n",
      "ImprovedBottleneck-58              [-1, 192, 64]               0\n",
      "        LeakyReLU-59              [-1, 192, 64]               0\n",
      "        MaxPool1d-60              [-1, 192, 21]               0\n",
      "        Dropout1d-61              [-1, 192, 21]               0\n",
      "           Conv1d-62               [-1, 64, 21]          12,352\n",
      "             LSTM-63            [[-1, 21, 128]]               0\n",
      "        Dropout1d-64              [-1, 21, 128]               0\n",
      "             LSTM-65            [[-1, 21, 128]]               0\n",
      "        Dropout1d-66                  [-1, 128]               0\n",
      "           Linear-67                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 110,617\n",
      "Trainable params: 110,617\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.75\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 2.18\n",
      "----------------------------------------------------------------\n",
      "outpus's shape : torch.Size([24, 1])\n"
     ]
    }
   ],
   "source": [
    "class SeEANet(nn.Module):\n",
    "    def __init__(self, PreLen=1, PreNum=1, input_shape = [256, 6]):\n",
    "        super(SeEANet, self).__init__()\n",
    "        firstFilter = 8\n",
    "        inceptionFilter = input_shape[1] * firstFilter\n",
    "        ImproverFilter = 4 * inceptionFilter\n",
    "\n",
    "        self.FirConv = FirstConvBlk(outch=firstFilter, input_shape=input_shape)\n",
    "        self.InceptionBlk = InceptionBlk(inch=inceptionFilter, outch=inceptionFilter)\n",
    "        self.lrelu1 = nn.LeakyReLU(negative_slope=0.3)\n",
    "        self.atten1 = ImprovedBottleneck(ImproverFilter=ImproverFilter, kernel=9, e=128, groups=8, alpha=1.0)\n",
    "        self.lrelu2 = nn.LeakyReLU(negative_slope=0.3)\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dp1 = nn.Dropout1d(0.2)\n",
    "        self.conv1 = nn.LazyConv1d(out_channels=64, kernel_size=1)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dp2 = nn.Dropout1d(0.2)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dp3 = nn.Dropout1d(0.2)\n",
    "        self.l = nn.LazyLinear(PreLen*PreNum)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.FirConv(x)\n",
    "        x = self.InceptionBlk(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.atten1(x)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.mpool1(x)\n",
    "        x = self.dp1(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x,_ = self.lstm1(x)\n",
    "        x = self.dp2(x)\n",
    "        x,_ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dp3(x)\n",
    "        y = self.l(x)\n",
    "\n",
    "\n",
    "        return y\n",
    "    \n",
    "model = SeEANet()\n",
    "model = model.cuda()\n",
    "inputs = torch.randn(24, 6, 256)\n",
    "inputs = inputs.cuda()\n",
    "summary(model, input_size=(6, 256))\n",
    "outputs = model(inputs)\n",
    "print(\"outpus's shape :\", outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 64, got 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# summary(model, input_size=(6, 256))\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutpus\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms shape :\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[95], line 34\u001b[0m, in \u001b[0;36mSeEANet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdp1(x)\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m---> 34\u001b[0m x,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdp2(x)\n\u001b[1;32m     36\u001b[0m x,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1101\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m   1095\u001b[0m         max_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m   1099\u001b[0m     )\n\u001b[1;32m   1100\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1002\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    999\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[1;32m   1001\u001b[0m ):\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1004\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1009\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1012\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/rnn.py:314\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 64, got 21"
     ]
    }
   ],
   "source": [
    "model = SeEANet()\n",
    "model = model.cuda()\n",
    "inputs = torch.randn(24, 6, 256)\n",
    "inputs = inputs.cuda()\n",
    "# summary(model, input_size=(6, 256))\n",
    "outputs = model(inputs)\n",
    "print(\"outpus's shape :\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outpus's shape : torch.Size([24, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "model = InceptionBlk(16, 16)\n",
    "model = model.cuda()\n",
    "inputs = torch.randn(24, 16, 256)\n",
    "inputs = inputs.cuda()\n",
    "# summary(model, input_size=(6, 256))\n",
    "outputs = model(inputs)\n",
    "print(\"outpus's shape :\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 256]             160\n",
      "         LeakyReLU-2              [-1, 16, 256]               0\n",
      "         MaxPool1d-3               [-1, 16, 64]               0\n",
      "           Dropout-4               [-1, 16, 64]               0\n",
      "            Conv1d-5              [-1, 16, 256]             160\n",
      "         LeakyReLU-6              [-1, 16, 256]               0\n",
      "         MaxPool1d-7               [-1, 16, 64]               0\n",
      "           Dropout-8               [-1, 16, 64]               0\n",
      "            Conv1d-9              [-1, 16, 256]             160\n",
      "        LeakyReLU-10              [-1, 16, 256]               0\n",
      "        MaxPool1d-11               [-1, 16, 64]               0\n",
      "          Dropout-12               [-1, 16, 64]               0\n",
      "           Conv1d-13              [-1, 16, 256]             160\n",
      "        LeakyReLU-14              [-1, 16, 256]               0\n",
      "        MaxPool1d-15               [-1, 16, 64]               0\n",
      "          Dropout-16               [-1, 16, 64]               0\n",
      "           Conv1d-17              [-1, 16, 256]             160\n",
      "        LeakyReLU-18              [-1, 16, 256]               0\n",
      "        MaxPool1d-19               [-1, 16, 64]               0\n",
      "          Dropout-20               [-1, 16, 64]               0\n",
      "           Conv1d-21              [-1, 16, 256]             160\n",
      "        LeakyReLU-22              [-1, 16, 256]               0\n",
      "        MaxPool1d-23               [-1, 16, 64]               0\n",
      "          Dropout-24               [-1, 16, 64]               0\n",
      "================================================================\n",
      "Total params: 960\n",
      "Trainable params: 960\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.47\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.48\n",
      "----------------------------------------------------------------\n",
      "outpus's shape : torch.Size([24, 96, 64])\n"
     ]
    }
   ],
   "source": [
    "model = FirstConvBlk(16)\n",
    "model = model.cuda()\n",
    "inputs = torch.randn(24, 6, 256)\n",
    "inputs = inputs.cuda()\n",
    "# summary(model, input_size=(6, 256))\n",
    "outputs = model(inputs)\n",
    "print(\"outpus's shape :\", outputs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
